{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUNILREDDY3/Colab-PyTorch-NeuralNetwork/blob/main/Practical_3_(16355183).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "njRXaWe0sjv7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "shuffle=True)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq5U7v8FuYnG",
        "outputId": "6e5c8f9b-e37d-41eb-adab-317cf8fffe2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.73MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 490kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.48MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.34MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN()"
      ],
      "metadata": {
        "id": "asYphZQHubLe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(3):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(trainloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y2fiyOxu_we",
        "outputId": "5d023d61-253f-4174-c324-0dbf41cb036e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.14658438668165333\n",
            "Epoch 2, Loss: 0.043459174211298586\n",
            "Epoch 3, Loss: 0.03058528918310005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)  # Total number of labels\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "# Print test accuracy\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao0NuFbfwORN",
        "outputId": "c495aba7-0d68-4f56-fcef-7469a52ffb21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 98.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "index = random.randint(0, len(images) - 1)\n",
        "img = images[index].squeeze()\n",
        "true_label = labels[index].item()\n",
        "output = model(images[index].unsqueeze(0))\n",
        "predicted_label = torch.argmax(output).item()\n",
        "plt.imshow(img.numpy(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_label}, True: {true_label}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "o66WNrpAyB7O",
        "outputId": "86074872-2fc8-43a9-88c8-85b35c3cfdf4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyDxS_7aVSnVsirX8hh0LKnUqDXq40-7XKg\"\n",
        "GEMINI_ENDPOINT = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "predicted_label = 5  # Ensure this value exists\n",
        "prompt = f\"The CNN model predicted digit {predicted_label} for an image. Explain why it might have made this prediction.\"\n",
        "\n",
        "# Correct API request format\n",
        "data = {\n",
        "    \"contents\": [{\"parts\": [{\"text\": prompt}]}]  # Gemini API expects this format\n",
        "}\n",
        "\n",
        "# Send API request\n",
        "response = requests.post(GEMINI_ENDPOINT, headers=headers, json=data)\n",
        "\n",
        "# Print full API response for debugging\n",
        "response_json = response.json()\n",
        "print(\"Full API Response:\", response_json)\n",
        "\n",
        "# Extract and print explanation if available\n",
        "if \"candidates\" in response_json:\n",
        "    explanation = response_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    print(\"Gemini Explanation:\", explanation)\n",
        "else:\n",
        "    print(\"Error:\", response_json.get(\"error\", \"Unknown error\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REzyOPymyGYV",
        "outputId": "5d4b02f9-6c30-4717-eef5-773810227c18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full API Response: {'candidates': [{'content': {'parts': [{'text': '**Possible Reasons for Predicting Digit 5:**\\n\\n* **Similar shape:** The digit 5 has a curved top and a straight or slightly curved bottom, which resembles the shape of certain curves or handwritten letters.\\n\\n* **Presence of \"hook\":** The digit 5 often features a small hook at the bottom-left, which could have been recognized by the model as a distinctive feature of 5.\\n\\n* **Occlusions:** If the image contained partial obstructions or noise, it could have distorted the shape of the digit, making it appear more similar to 5.\\n\\n* **Segmentation errors:** The model might have incorrectly segmented the image into separate regions, leading it to misinterpret the actual digit as a combination of shapes that resemble 5.\\n\\n* **Confusion with other digits:** Certain digits, such as 2 and 6, can have similar shapes under certain conditions. The model might have misclassified the digit based on subtle differences that were not accurately captured by its features.\\n\\n* **Model bias:** If the model was trained on a dataset with an overrepresentation of digit 5, it may have developed a bias towards predicting 5 when presented with ambiguous or noisy images.\\n\\n* **Insufficient data:** The model may have been trained on a dataset that did not provide enough examples of digits in various orientations and variations. This could have limited its ability to make accurate predictions when encountering uncommon or distorted images.\\n\\n* **Model architecture limitations:** The architecture of the CNN model used may not have been sufficiently complex or optimized to handle the task of digit recognition accurately.'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0, 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}], 'usageMetadata': {'promptTokenCount': 20, 'candidatesTokenCount': 318, 'totalTokenCount': 338}, 'modelVersion': 'gemini-pro'}\n",
            "Gemini Explanation: **Possible Reasons for Predicting Digit 5:**\n",
            "\n",
            "* **Similar shape:** The digit 5 has a curved top and a straight or slightly curved bottom, which resembles the shape of certain curves or handwritten letters.\n",
            "\n",
            "* **Presence of \"hook\":** The digit 5 often features a small hook at the bottom-left, which could have been recognized by the model as a distinctive feature of 5.\n",
            "\n",
            "* **Occlusions:** If the image contained partial obstructions or noise, it could have distorted the shape of the digit, making it appear more similar to 5.\n",
            "\n",
            "* **Segmentation errors:** The model might have incorrectly segmented the image into separate regions, leading it to misinterpret the actual digit as a combination of shapes that resemble 5.\n",
            "\n",
            "* **Confusion with other digits:** Certain digits, such as 2 and 6, can have similar shapes under certain conditions. The model might have misclassified the digit based on subtle differences that were not accurately captured by its features.\n",
            "\n",
            "* **Model bias:** If the model was trained on a dataset with an overrepresentation of digit 5, it may have developed a bias towards predicting 5 when presented with ambiguous or noisy images.\n",
            "\n",
            "* **Insufficient data:** The model may have been trained on a dataset that did not provide enough examples of digits in various orientations and variations. This could have limited its ability to make accurate predictions when encountering uncommon or distorted images.\n",
            "\n",
            "* **Model architecture limitations:** The architecture of the CNN model used may not have been sufficiently complex or optimized to handle the task of digit recognition accurately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFARCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model_cifar = CIFARCNN()"
      ],
      "metadata": {
        "id": "nnmgQDVWY4wz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for RGB images\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class CIFARCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFARCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
        "model = CIFARCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train for 2 epochs\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move data to GPU if available\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)  # Move to GPU if available\n",
        "        outputs = model(images)  # Forward pass\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Print final test accuracy\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyT2bgAKZTYM",
        "outputId": "8234656e-eff2-47e1-b3ee-da67d2abf70c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1/2, Loss: 1.4840\n",
            "Epoch 2/2, Loss: 1.1295\n",
            "Test Accuracy: 62.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YslfAMY6aXrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST is a simpler dataset, consisting of grayscale images of handwritten digits (28x28 pixels) with low variability. This makes it easier to train shallow CNN models with just a few layers, achieving high accuracy (~98-99%) in a short time (e.g., 3 epochs). The small input size and lack of color or complex backgrounds reduce computational demands and the risk of overfitting, making MNIST ideal for beginners and quick experimentation.\n",
        "\n",
        "In contrast, CIFAR-10 is more complex, featuring RGB images of 10 object classes (32x32 pixels) with high variability in color, texture, and orientation. Training CNNs for CIFAR-10 requires deeper architectures, regularization techniques, and more epochs to achieve moderate accuracy (~70-85%). The increased complexity and computational demands make CIFAR-10 a more challenging benchmark for testing advanced deep learning techniques."
      ],
      "metadata": {
        "id": "EFbQbzBMmIS_"
      }
    }
  ]
}